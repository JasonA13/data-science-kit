# Import Modules - versions used were TF 1.10.0 and Keras 2.1.6

%matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Embedding, GRU, Flatten
from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras import backend as K


# Import Data - read in dataset and return first rows

Read in csv file and return first rows.
# Enter location full_data = pd.read_csv('...')
full_data.head(10)


# Initial Text Preprocessing and Dataset Splits
# Convert all text to lower case, partition score, and split dataset into training and test sets

corpus=[]
for text in full_data:
    text=text.lower()
    corpus.append(text)

df = full_data[['Score', 'Text']]

def partition(x):
    if x < 3:
        return 0.0
    return 1.0
    
Score=df['Score']
Score=Score.map(partition)
Text=df['Text']

tmp=df
tmp['Score']=tmp['Score'].map(partition)
tmp.head()

X_train, X_test, y_train, y_test = train_test_split(Text, Score, test_size=0.2, random_state=42)


# Tokenize text and restrict dictionary to most common words

num_words = 10000
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(X_train)
tokenizer.word_index
X_train_tokens = tokenizer.texts_to_sequences(X_train)
X_test_tokens = tokenizer.texts_to_sequences(X_test)
X_train[1]
np.array(X_train_tokens[1])

#Padding and Truncating - Generate sequences of equal length based on mean plus two SDs.

num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens]
num_tokens = np.array(num_tokens)
np.mean(num_tokens)
np.max(num_tokens)
max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)
max_tokens = int(max_tokens)
max_tokens
np.sum(num_tokens < max_tokens) / len(num_tokens)

pad = 'pre'
X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens,
                            padding=pad, truncating=pad)
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens,
                           padding=pad, truncating=pad)

X_train_pad.shape
X_test_pad.shape


# Inverse mapping - Define function to convert tokens back to text

idx = tokenizer.word_index
inverse_map = dict(zip(idx.values(), idx.keys()))
def tokens_to_string(tokens):
    # Map from tokens back to words.
    words = [inverse_map[token] for token in tokens if token != 0]
    
    # Concatenate all words.
    text = " ".join(words)

    return text

tokens_to_string(X_train_tokens[1])


# RNN Model - Create classifier with recurrent layers using Keras Sequential model API

model = Sequential()
embedding_size = 8
model.add(Embedding(input_dim=num_words,
                    output_dim=embedding_size,
                    input_length=max_tokens,
                    name='layer_embedding'))

model.add(GRU(units=16, return_sequences=True))
model.add(GRU(units=8, return_sequences=True))
model.add(GRU(units=4))
model.add(Dense(1, activation='sigmoid'))
optimizer = Adam(lr=1e-3)
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
model.summary()


# RNN - Training and Evaluation - Train the model and perform validation using 5 percent of the training set
%%time
model.fit(X_train_pad, y_train,
          validation_split=0.05, epochs=6, batch_size=64)
%%time
result = model.evaluate(X_test_pad, y_test)
print("Accuracy: {0:.2%}".format(result[1]))
